"""
Task Evaluation System for LLM-driven World Model Experiment

ã‚¿ã‚¹ã‚¯è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ï¼šå ±é…¬è¨ˆç®—ã€æˆåŠŸ/å¤±æ•—åˆ¤å®šã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
"""

import math
from dataclasses import dataclass, field
from enum import StrEnum

from pydantic import BaseModel, Field

# ==================== Data Models ====================


class TaskLevel(StrEnum):
    """ã‚¿ã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«"""

    TUTORIAL = "tutorial"
    FRICTION = "friction"
    OBSTACLE = "obstacle"
    BARRIER = "barrier"


class ActionType(StrEnum):
    """è¡Œå‹•ã‚¿ã‚¤ãƒ—"""

    PUSH = "push"
    BARRIER = "barrier"
    WAIT = "wait"
    OBSERVE = "observe"


class Point(BaseModel):
    """2Dåº§æ¨™"""

    x: float
    y: float


class Velocity(BaseModel):
    """é€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ«"""

    x: float
    y: float


class BoxState(BaseModel):
    """è·ç‰©ã®çŠ¶æ…‹"""

    position: Point
    velocity: Velocity
    mass: float
    friction: float
    restitution: float


class GoalState(BaseModel):
    """ã‚´ãƒ¼ãƒ«ã®çŠ¶æ…‹"""

    position: Point
    radius: float = 30.0


class Action(BaseModel):
    """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¡Œå‹•"""

    type: ActionType
    force_x: float | None = Field(None, alias="forceX")
    force_y: float | None = Field(None, alias="forceY")
    duration: float | None = None
    x: float | None = None
    y: float | None = None
    angle: float | None = None
    reason: str | None = None
    focus: str | None = None

    class Config:
        populate_by_name = True  # Accept both snake_case and camelCase


class SimulationState(BaseModel):
    """ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çŠ¶æ…‹"""

    box: BoxState
    goal: GoalState
    barriers: list[dict] = Field(default_factory=list)
    step: int = 0
    elapsed_time: float = 0.0
    actions_taken: list[Action] = Field(default_factory=list)


class TaskOutcome(BaseModel):
    """ã‚¿ã‚¹ã‚¯çµæœ"""

    # æˆåŠŸ/å¤±æ•—
    goal_reached: bool = False
    box_out_of_bounds: bool = False
    timeout: bool = False

    # è·é›¢
    initial_distance: float
    final_distance: float
    distance_improvement: float

    # åŠ¹ç‡æ€§
    steps: int
    total_force: float = 0.0
    excessive_force: bool = False

    # å‹•ã
    smooth_movement: bool = True
    path_length: float = 0.0

    # æˆ¦ç•¥
    strategy_hash: str = ""
    is_novel_strategy: bool = False

    # æ™‚é–“
    completion_time: float = 0.0


class EpisodeMetrics(BaseModel):
    """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è©•ä¾¡æŒ‡æ¨™"""

    # æˆåŠŸç‡
    success_rate: float = 0.0
    success_count: int = 0
    total_episodes: int = 0

    # åŠ¹ç‡æ€§
    avg_steps_to_goal: float = 0.0
    avg_force_magnitude: float = 0.0
    avg_completion_time: float = 0.0

    # å­¦ç¿’
    unique_strategies: int = 0
    strategy_diversity: float = 0.0

    # å‰µç™ºæ€§
    novel_actions: int = 0
    unexpected_solutions: int = 0

    # å“è³ª
    smoothness_score: float = 0.0
    elegance_score: float = 0.0

    # å ±é…¬
    total_reward: float = 0.0
    avg_reward: float = 0.0
    max_reward: float = 0.0


# ==================== Level Configuration ====================


@dataclass
class LevelConfig:
    """ãƒ¬ãƒ™ãƒ«è¨­å®š"""

    name: str
    description: str
    box_position: Point
    goal_position: Point
    box_mass: float = 10.0
    friction: float = 0.5
    restitution: float = 0.3
    time_limit: float = 30.0
    max_steps: int = 50
    available_barriers: int = 0
    obstacles: list[dict] = field(default_factory=list)


# äº‹å‰å®šç¾©ãƒ¬ãƒ™ãƒ«
LEVELS = {
    TaskLevel.TUTORIAL: LevelConfig(
        name="åŸºç¤ï¼šç›´ç·šç§»å‹•",
        description="è·ç‰©ã‚’å³ã«æŠ¼ã—ã¦ã‚´ãƒ¼ãƒ«ã¸",
        box_position=Point(x=200, y=300),
        goal_position=Point(x=600, y=300),
        box_mass=10.0,
        friction=0.5,
        time_limit=60.0,  # AI mode needs more time (2s per step * 20 steps = 40s minimum)
        max_steps=20,
    ),
    TaskLevel.FRICTION: LevelConfig(
        name="ç‰©ç†ï¼šæ‘©æ“¦ä¿‚æ•°",
        description="æ»‘ã‚Šã‚„ã™ã„è·ç‰©ã‚’ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«",
        box_position=Point(x=200, y=300),
        goal_position=Point(x=600, y=300),
        box_mass=10.0,
        friction=0.1,  # éå¸¸ã«æ»‘ã‚‹
        time_limit=80.0,  # AI mode needs more time
        max_steps=30,
    ),
    TaskLevel.OBSTACLE: LevelConfig(
        name="éšœå®³ï¼šå£ã®å›é¿",
        description="å£ã‚’é¿ã‘ã¦ã‚´ãƒ¼ãƒ«ã¸",
        box_position=Point(x=200, y=300),
        goal_position=Point(x=600, y=300),
        box_mass=10.0,
        friction=0.5,
        time_limit=100.0,  # AI mode needs more time
        max_steps=40,
        obstacles=[{"type": "wall", "x": 400, "y": 200, "width": 20, "height": 400}],
    ),
    TaskLevel.BARRIER: LevelConfig(
        name="æˆ¦ç•¥ï¼šèª˜å°è·¯ä½œæˆ",
        description="ãƒãƒªã‚¢ã§æ»‘ã‚Šå°ã‚’ä½œã‚Šã€å®‰å…¨ã«ã‚´ãƒ¼ãƒ«ã¸",
        box_position=Point(x=200, y=100),  # é«˜ã„ä½ç½®
        goal_position=Point(x=600, y=500),
        box_mass=10.0,
        friction=0.3,
        time_limit=120.0,  # AI mode needs more time
        max_steps=50,
        available_barriers=3,
        obstacles=[{"type": "pit", "x": 400, "y": 550, "width": 100, "height": 50}],
    ),
}


# ==================== Reward Calculator ====================


class RewardCalculator:
    """å ±é…¬è¨ˆç®—å™¨"""

    def __init__(self):
        self.past_strategies: set = set()

    def calculate(self, outcome: TaskOutcome, level: LevelConfig) -> float:
        """
        å ±é…¬ã‚’è¨ˆç®—

        Args:
            outcome: ã‚¿ã‚¹ã‚¯çµæœ
            level: ãƒ¬ãƒ™ãƒ«è¨­å®š

        Returns:
            å ±é…¬å€¤
        """
        reward = 0.0

        # ã€ä¸»è¦ç›®æ¨™ã€‘ã‚´ãƒ¼ãƒ«åˆ°é”
        if outcome.goal_reached:
            reward += 100

            # åŠ¹ç‡æ€§ãƒœãƒ¼ãƒŠã‚¹
            time_bonus = max(0, 50 - outcome.steps * 2)
            reward += time_bonus

            # å„ªé›…ã•ãƒœãƒ¼ãƒŠã‚¹
            if outcome.smooth_movement:
                reward += 20

        # ã€é€²æ—è©•ä¾¡ã€‘è·é›¢ã®æ¸›å°‘
        distance_improvement = outcome.initial_distance - outcome.final_distance
        reward += distance_improvement * 0.1  # ä¿‚æ•°ã‚’0.5ã‹ã‚‰0.1ã«èª¿æ•´

        # ã€å‰µç™ºæ€§ãƒœãƒ¼ãƒŠã‚¹ã€‘æ–°ã—ã„æˆ¦ç•¥
        if outcome.is_novel_strategy:
            reward += 30
            print(f"ğŸ‰ æ–°æˆ¦ç•¥ç™ºè¦‹! Hash: {outcome.strategy_hash[:8]}")

        # ã€ãƒšãƒŠãƒ«ãƒ†ã‚£ã€‘
        if outcome.box_out_of_bounds:
            reward -= 50

        if outcome.excessive_force:
            reward -= 10

        if outcome.timeout:
            reward -= 20

        return reward

    def register_strategy(self, strategy_hash: str):
        """æˆ¦ç•¥ã‚’ç™»éŒ²"""
        self.past_strategies.add(strategy_hash)

    def is_novel_strategy(self, strategy_hash: str) -> bool:
        """æ–°è¦æˆ¦ç•¥ã‹ã©ã†ã‹"""
        return strategy_hash not in self.past_strategies


# ==================== Task Evaluator ====================


class TaskEvaluator:
    """ã‚¿ã‚¹ã‚¯è©•ä¾¡å™¨"""

    def __init__(self):
        self.reward_calculator = RewardCalculator()
        self.metrics = EpisodeMetrics()

    def evaluate_state(self, state: SimulationState, level: LevelConfig) -> tuple[bool, bool, str]:
        """
        ç¾åœ¨ã®çŠ¶æ…‹ã‚’è©•ä¾¡

        Args:
            state: ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çŠ¶æ…‹
            level: ãƒ¬ãƒ™ãƒ«è¨­å®š

        Returns:
            (æˆåŠŸ, å¤±æ•—, ç†ç”±)
        """
        # æˆåŠŸåˆ¤å®š
        if self.is_success(state.box.position, state.goal.position):
            return True, False, "ã‚´ãƒ¼ãƒ«åˆ°é”"

        # å¤±æ•—åˆ¤å®š
        if state.box.position.y > 600:
            return False, True, "ç”»é¢å¤–ã«è½ä¸‹"

        if state.step >= level.max_steps:
            return False, True, "åˆ¶é™ã‚¹ãƒ†ãƒƒãƒ—è¶…é"

        if state.box.position.x < -100 or state.box.position.x > 900:
            return False, True, "ç”»é¢å¤–ï¼ˆæ¨ªæ–¹å‘ï¼‰"

        if state.elapsed_time > level.time_limit:
            return False, True, "æ™‚é–“åˆ‡ã‚Œ"

        return False, False, "ç¶™ç¶šä¸­"

    def is_success(self, box_pos: Point, goal_pos: Point) -> bool:
        """æˆåŠŸåˆ¤å®š"""
        distance = self.euclidean_distance(box_pos, goal_pos)
        return distance < 30.0

    def evaluate_episode(
        self,
        initial_state: SimulationState,
        final_state: SimulationState,
        level: LevelConfig,
    ) -> tuple[TaskOutcome, float]:
        """
        ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å…¨ä½“ã‚’è©•ä¾¡

        Args:
            initial_state: åˆæœŸçŠ¶æ…‹
            final_state: æœ€çµ‚çŠ¶æ…‹
            level: ãƒ¬ãƒ™ãƒ«è¨­å®š

        Returns:
            (ã‚¿ã‚¹ã‚¯çµæœ, å ±é…¬)
        """
        initial_dist = self.euclidean_distance(
            initial_state.box.position, initial_state.goal.position
        )
        final_dist = self.euclidean_distance(final_state.box.position, final_state.goal.position)

        # è¡Œå‹•ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‹ã‚‰ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆ
        strategy_hash = self.compute_strategy_hash(final_state.actions_taken)

        # åŠ›ã®åˆè¨ˆã‚’è¨ˆç®—
        total_force = sum(
            math.sqrt((a.force_x or 0) ** 2 + (a.force_y or 0) ** 2)
            for a in final_state.actions_taken
            if a.type == ActionType.PUSH
        )

        # å‹•ãã®æ»‘ã‚‰ã‹ã•ã‚’è©•ä¾¡
        smooth_movement = self.evaluate_smoothness(final_state.actions_taken)

        # ã‚¿ã‚¹ã‚¯çµæœã‚’æ§‹ç¯‰
        outcome = TaskOutcome(
            goal_reached=self.is_success(final_state.box.position, final_state.goal.position),
            box_out_of_bounds=(
                final_state.box.position.y > 600
                or final_state.box.position.x < -100
                or final_state.box.position.x > 900
            ),
            timeout=(final_state.elapsed_time > level.time_limit),
            initial_distance=initial_dist,
            final_distance=final_dist,
            distance_improvement=initial_dist - final_dist,
            steps=final_state.step,
            total_force=total_force,
            excessive_force=(total_force > 5.0),
            smooth_movement=smooth_movement,
            strategy_hash=strategy_hash,
            is_novel_strategy=self.reward_calculator.is_novel_strategy(strategy_hash),
            completion_time=final_state.elapsed_time,
        )

        # å ±é…¬ã‚’è¨ˆç®—
        reward = self.reward_calculator.calculate(outcome, level)

        # æ–°è¦æˆ¦ç•¥ã‚’ç™»éŒ²
        if outcome.is_novel_strategy:
            self.reward_calculator.register_strategy(strategy_hash)

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ›´æ–°
        self.update_metrics(outcome, reward)

        return outcome, reward

    def compute_strategy_hash(self, actions: list[Action]) -> str:
        """è¡Œå‹•ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‹ã‚‰ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆ"""
        # è¡Œå‹•ã‚¿ã‚¤ãƒ—ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ãƒãƒƒã‚·ãƒ¥åŒ–
        action_sequence = "".join([a.type.value[0] for a in actions])  # p, b, w, o

        # åŠ›ã®æ–¹å‘ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚è€ƒæ…®
        force_pattern = ""
        for a in actions:
            if a.type == ActionType.PUSH:
                if a.force_x and a.force_x > 0:
                    force_pattern += "R"  # Right
                elif a.force_x and a.force_x < 0:
                    force_pattern += "L"  # Left
                if a.force_y and a.force_y > 0:
                    force_pattern += "D"  # Down
                elif a.force_y and a.force_y < 0:
                    force_pattern += "U"  # Up

        combined = f"{action_sequence}:{force_pattern}"
        return str(hash(combined))

    def evaluate_smoothness(self, actions: list[Action]) -> bool:
        """å‹•ãã®æ»‘ã‚‰ã‹ã•ã‚’è©•ä¾¡"""
        if len(actions) < 2:
            return True

        # åŠ›ã®å¤‰åŒ–ãŒæ€¥æ¿€ã§ãªã„ã‹
        push_actions = [a for a in actions if a.type == ActionType.PUSH]
        if len(push_actions) < 2:
            return True

        max_force_change = 0.0
        for i in range(1, len(push_actions)):
            prev = push_actions[i - 1]
            curr = push_actions[i]

            prev_mag = math.sqrt((prev.force_x or 0) ** 2 + (prev.force_y or 0) ** 2)
            curr_mag = math.sqrt((curr.force_x or 0) ** 2 + (curr.force_y or 0) ** 2)

            force_change = abs(curr_mag - prev_mag)
            max_force_change = max(max_force_change, force_change)

        # å¤‰åŒ–ãŒ0.1ä»¥ä¸‹ãªã‚‰æ»‘ã‚‰ã‹
        return max_force_change < 0.1

    def update_metrics(self, outcome: TaskOutcome, reward: float):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ›´æ–°"""
        self.metrics.total_episodes += 1

        if outcome.goal_reached:
            self.metrics.success_count += 1

        self.metrics.success_rate = self.metrics.success_count / self.metrics.total_episodes

        # å¹³å‡å€¤ã®æ›´æ–°ï¼ˆç´¯ç©å¹³å‡ï¼‰
        n = self.metrics.total_episodes
        self.metrics.avg_steps_to_goal = (
            self.metrics.avg_steps_to_goal * (n - 1) + outcome.steps
        ) / n
        self.metrics.avg_force_magnitude = (
            self.metrics.avg_force_magnitude * (n - 1) + outcome.total_force
        ) / n
        self.metrics.avg_completion_time = (
            self.metrics.avg_completion_time * (n - 1) + outcome.completion_time
        ) / n

        # å ±é…¬
        self.metrics.total_reward += reward
        self.metrics.avg_reward = self.metrics.total_reward / n
        self.metrics.max_reward = max(self.metrics.max_reward, reward)

        # å‰µç™ºæ€§
        if outcome.is_novel_strategy:
            self.metrics.novel_actions += 1
            self.metrics.unique_strategies = len(self.reward_calculator.past_strategies)

        # å“è³ª
        if outcome.smooth_movement:
            self.metrics.smoothness_score = (self.metrics.smoothness_score * (n - 1) + 1.0) / n
        else:
            self.metrics.smoothness_score = (self.metrics.smoothness_score * (n - 1) + 0.0) / n

    def euclidean_distance(self, p1: Point, p2: Point) -> float:
        """ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢"""
        return math.sqrt((p1.x - p2.x) ** 2 + (p1.y - p2.y) ** 2)

    def get_metrics(self) -> EpisodeMetrics:
        """ç¾åœ¨ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—"""
        return self.metrics

    def reset_metrics(self):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆ"""
        self.metrics = EpisodeMetrics()


# ==================== Helper Functions ====================


def get_level_config(level: TaskLevel) -> LevelConfig:
    """ãƒ¬ãƒ™ãƒ«è¨­å®šã‚’å–å¾—"""
    return LEVELS[level]


def create_initial_state(level: LevelConfig) -> SimulationState:
    """åˆæœŸçŠ¶æ…‹ã‚’ä½œæˆ"""
    return SimulationState(
        box=BoxState(
            position=level.box_position,
            velocity=Velocity(x=0, y=0),
            mass=level.box_mass,
            friction=level.friction,
            restitution=level.restitution,
        ),
        goal=GoalState(position=level.goal_position),
        barriers=[],
        step=0,
        elapsed_time=0.0,
        actions_taken=[],
    )
