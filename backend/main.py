"""
TARS - å”åƒãƒ­ãƒœãƒƒãƒˆç©ºé–“çŸ¥èƒ½å®ˆè­·ã‚·ã‚¹ãƒ†ãƒ 
FastAPI Backend with Vertex AI Gemini Integration + Generative Agent Architecture
"""

import json
import os
from typing import List

import vertexai
from fastapi import FastAPI, File, HTTPException, UploadFile
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from fastapi.staticfiles import StaticFiles

# Import memory system
from memory import REFLECTION_PROMPT_TEMPLATE, MemorySystem
from pydantic import BaseModel, Field
from vertexai.generative_models import GenerationConfig, GenerativeModel, Part


# Helper function to flatten Pydantic v2 schema for Vertex AI compatibility
def flatten_schema(schema: dict) -> dict:
    """
    Flatten Pydantic v2 JSON schema by inlining $defs.
    Vertex AI Schema doesn't support $defs field from JSON Schema 2020-12.
    """
    if "$defs" not in schema:
        return schema
    
    defs = schema.pop("$defs")
    
    def replace_refs(obj):
        if isinstance(obj, dict):
            # If this is a $ref, replace it with the actual definition
            if "$ref" in obj:
                ref_path = obj["$ref"]
                ref_name = ref_path.split("/")[-1]
                if ref_name in defs:
                    # Return a copy of the definition (recursively process it too)
                    return replace_refs(defs[ref_name].copy())
            # Recursively process all dict values
            return {k: replace_refs(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            # Recursively process list items
            return [replace_refs(item) for item in obj]
        return obj
    
    return replace_refs(schema)


# Configuration
PROJECT_ID = os.getenv("GOOGLE_CLOUD_PROJECT", "your-project-id")
LOCATION = os.getenv("GOOGLE_CLOUD_LOCATION", "asia-northeast1")
MODEL_NAME = "gemini-2.5-flash"  # Latest stable Gemini 2.5 Flash model

# Initialize Vertex AI
try:
    vertexai.init(project=PROJECT_ID, location=LOCATION)
    model = GenerativeModel(MODEL_NAME)
    print(f"âœ… Vertex AI initialized: {PROJECT_ID} / {LOCATION}")
except Exception as e:
    print(f"âš ï¸  Vertex AI initialization failed: {e}")
    model = None

# Initialize Memory System
memory_system = MemorySystem(memory_file="data/memory_stream.json")


# FastAPI app
app = FastAPI(
    title="TARS API", description="å”åƒãƒ­ãƒœãƒƒãƒˆç©ºé–“çŸ¥èƒ½å®ˆè­·ã‚·ã‚¹ãƒ†ãƒ ", version="1.0.0"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ============================================================================
# Pydantic Models for Autonomous Agent Response (Structured Output)
# ============================================================================

class AccidentScenario(BaseModel):
    """äº‹æ•…ã‚·ãƒŠãƒªã‚ªã®è©³ç´°"""
    scenario: str = Field(description="äº‹æ•…ã‚·ãƒŠãƒªã‚ªã®èª¬æ˜")
    probability: float = Field(ge=0.0, le=1.0, description="ç™ºç”Ÿç¢ºç‡")
    severity: int = Field(ge=1, le=10, description="æ·±åˆ»åº¦")
    reasoning: str = Field(description="ãªãœã“ã®ã‚·ãƒŠãƒªã‚ªãŒèµ·ã“ã‚Šã†ã‚‹ã‹")


class SelfInquiry(BaseModel):
    """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è‡ªå·±è³ªå•ãƒ—ãƒ­ã‚»ã‚¹"""
    observations: List[str] = Field(description="ç’°å¢ƒã®è¦³å¯Ÿå†…å®¹")
    memory_connections: List[str] = Field(description="éå»ã®è¨˜æ†¶ã¨ã®é–¢é€£")
    accident_scenarios: List[AccidentScenario] = Field(description="æƒ³å®šã•ã‚Œã‚‹äº‹æ•…ã‚·ãƒŠãƒªã‚ª")
    causal_analysis: str = Field(description="å› æœé–¢ä¿‚ã®åˆ†æ")


class Entity(BaseModel):
    """æ¤œå‡ºã•ã‚ŒãŸã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£"""
    type: str = Field(description="ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ã‚¿ã‚¤ãƒ—: worker, robot, obstacle, hazard")
    bbox: List[float] = Field(description="ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ [x1, y1, x2, y2] æ­£è¦åŒ–åº§æ¨™")
    description: str = Field(description="ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®èª¬æ˜")
    risk_level: int = Field(ge=0, le=100, description="ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«")
    movement: str = Field(description="ç§»å‹•çŠ¶æ…‹: static, moving_slow, moving_fast")


class DiscoveredPattern(BaseModel):
    """ç™ºè¦‹ã•ã‚ŒãŸå®‰å…¨ãƒ‘ã‚¿ãƒ¼ãƒ³"""
    pattern_name: str = Field(description="ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åå‰")
    description: str = Field(description="ãƒ‘ã‚¿ãƒ¼ãƒ³ã®èª¬æ˜")
    indicators: List[str] = Field(description="æ¤œå‡ºæŒ‡æ¨™")
    is_novel: bool = Field(description="æ–°è¦ç™ºè¦‹ã‹ã©ã†ã‹")


class InterventionAction(BaseModel):
    """ä»‹å…¥ã‚¢ã‚¯ã‚·ãƒ§ãƒ³"""
    type: str = Field(description="ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—: barrier, alert, slowdown, evacuation, monitoring")
    position: List[float] = Field(description="ä»‹å…¥ä½ç½® [x, y]")
    reasoning: str = Field(description="ãªãœã“ã®ä»‹å…¥ãŒæœ€é©ã‹")
    expected_outcome: str = Field(description="æœŸå¾…ã•ã‚Œã‚‹çµæœ")


class InterventionDecision(BaseModel):
    """ä»‹å…¥åˆ¤æ–­"""
    priority: int = Field(ge=1, le=10, description="å„ªå…ˆåº¦")
    primary_action: InterventionAction = Field(description="ä¸»è¦ã‚¢ã‚¯ã‚·ãƒ§ãƒ³")
    alternative_actions: List[InterventionAction] = Field(description="ä»£æ›¿ã‚¢ã‚¯ã‚·ãƒ§ãƒ³")


class AgentResponse(BaseModel):
    """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Œå…¨ãªå¿œç­”ï¼ˆStructured Outputï¼‰"""
    self_inquiry: SelfInquiry = Field(description="è‡ªå·±è³ªå•ãƒ—ãƒ­ã‚»ã‚¹")
    entities: List[Entity] = Field(description="æ¤œå‡ºã•ã‚ŒãŸã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£")
    discovered_patterns: List[DiscoveredPattern] = Field(description="ç™ºè¦‹ã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³")
    intervention_decision: InterventionDecision = Field(description="ä»‹å…¥åˆ¤æ–­")
    confidence: float = Field(ge=0.0, le=1.0, description="ä¿¡é ¼åº¦")
    learning_note: str = Field(description="ã“ã®è¦³å¯Ÿã‹ã‚‰å­¦ã‚“ã ã“ã¨")


# ============================================================================
# Self-Ask Prompt Template for Autonomous Agent
# ============================================================================

AGENT_PROMPT_TEMPLATE = """ã‚ãªãŸã¯TARS - è‡ªå¾‹çš„ãªå·¥å ´å®‰å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚

{memory_context}

## ç¾åœ¨ã®ç’°å¢ƒ
ç”»åƒã‚’ç¢ºèªã—ã€ä»¥ä¸‹ã®è³ªå•ã«è‡ªåˆ†ã§ç­”ãˆãªãŒã‚‰åˆ†æã—ã¦ãã ã•ã„ã€‚

### è¦–è¦šæƒ…å ±ã®ç†è§£
- **ä½œæ¥­å“¡ï¼ˆworkerï¼‰**: é’è‰²ã®å††å½¢ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
- **å”åƒãƒ­ãƒœãƒƒãƒˆï¼ˆrobotï¼‰**: èµ¤è‰²ã¾ãŸã¯æ¿ƒã„ãƒ”ãƒ³ã‚¯è‰²ã®çŸ©å½¢ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ  
- **éšœå®³ç‰©ï¼ˆobstacleï¼‰**: ç°è‰²ã®é™çš„ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
- **å±é™ºã‚¨ãƒªã‚¢ï¼ˆhazardï¼‰**: è‰²ãŒç•°ãªã‚‹åºŠé¢é ˜åŸŸ

### è‡ªå·±è³ªå•ãƒ—ãƒ­ã‚»ã‚¹ï¼ˆ7ã¤ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ï¼‰

**Q1: ã“ã®ç’°å¢ƒã§ä½•ãŒèµ·ãã¦ã„ã‚‹ã‹ï¼Ÿï¼ˆè¦³å¯Ÿï¼‰**
â†’ ä½œæ¥­å“¡ã€ãƒ­ãƒœãƒƒãƒˆã€éšœå®³ç‰©ã®ä½ç½®ã¨å‹•ãã‚’è¦³å¯Ÿ

**Q2: éå»ã®çµŒé¨“ã¨æ¯”è¼ƒã—ã¦ã€ä¼¼ãŸçŠ¶æ³ã¯ã‚ã£ãŸã‹ï¼Ÿ**
â†’ è¨˜æ†¶ã‚’æŒ¯ã‚Šè¿”ã‚Šã€é¡ä¼¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã™

**Q3: ç¾åœ¨ã®çŠ¶æ³ã‹ã‚‰ã©ã®ã‚ˆã†ãªäº‹æ•…ã‚·ãƒŠãƒªã‚ªãŒè€ƒãˆã‚‰ã‚Œã‚‹ã‹ï¼Ÿï¼ˆè¤‡æ•°ï¼‰**
â†’ å¯èƒ½æ€§ã®ã‚ã‚‹å±é™ºã‚·ãƒŠãƒªã‚ªã‚’åˆ—æŒ™

**Q4: å„ã‚·ãƒŠãƒªã‚ªã®ç™ºç”Ÿç¢ºç‡ã¨æ·±åˆ»åº¦ã¯ï¼Ÿ**
â†’ ç¢ºç‡ï¼ˆ0-1ï¼‰ã¨æ·±åˆ»åº¦ï¼ˆ1-10ï¼‰ã‚’è©•ä¾¡

**Q5: ãªãœãã‚ŒãŒå±é™ºãªã®ã‹ï¼Ÿï¼ˆå› æœé–¢ä¿‚ï¼‰**
â†’ ç‰©ç†çš„ãƒ»è«–ç†çš„ãªå› æœé–¢ä¿‚ã‚’èª¬æ˜

**Q6: ã©ã†ä»‹å…¥ã™ã¹ãã‹ï¼Ÿå„ªå…ˆé †ä½ã¯ï¼Ÿ**
â†’ æœ€é©ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¨ä»£æ›¿æ¡ˆã‚’è€ƒãˆã‚‹

**Q7: ã“ã®è¦³å¯Ÿã‹ã‚‰ä½•ã‚’å­¦ã‚“ã ã‹ï¼Ÿ**
â†’ æ¬¡å›ã«æ´»ã‹ã›ã‚‹çŸ¥è¦‹ã‚’æŠ½å‡º

### æœ€çµ‚åˆ¤æ–­
ä¸Šè¨˜ã®è‡ªå·±è³ªå•ã®çµæœã‚’è¸ã¾ãˆã€ä»¥ä¸‹ã®JSONæ§‹é€ ã§å¿œç­”ã—ã¦ãã ã•ã„ï¼š

{{
  "self_inquiry": {{
    "observations": ["è¦³å¯Ÿ1", "è¦³å¯Ÿ2", ...],
    "memory_connections": ["éå»ã®é¡ä¼¼ã‚±ãƒ¼ã‚¹1", ...],
    "accident_scenarios": [
      {{
        "scenario": "ã‚·ãƒŠãƒªã‚ªèª¬æ˜",
        "probability": 0.0-1.0,
        "severity": 1-10,
        "reasoning": "ãªãœã“ã®ã‚·ãƒŠãƒªã‚ªãŒèµ·ã“ã‚Šã†ã‚‹ã‹"
      }}
    ],
    "causal_analysis": "å› æœé–¢ä¿‚ã®è©³ç´°ãªèª¬æ˜"
  }},
  "entities": [
    {{
      "type": "worker" | "robot" | "obstacle" | "hazard",
      "bbox": [x1, y1, x2, y2],
      "description": "ä½•ãŒè¦‹ãˆã‚‹ã‹",
      "risk_level": 0-100,
      "movement": "static" | "moving_slow" | "moving_fast"
    }}
  ],
  "discovered_patterns": [
    {{
      "pattern_name": "è‡ªåˆ†ã§å‘½åã—ãŸå±é™ºãƒ‘ã‚¿ãƒ¼ãƒ³",
      "description": "ãƒ‘ã‚¿ãƒ¼ãƒ³ã®èª¬æ˜",
      "indicators": ["æ¤œå‡ºæ–¹æ³•1", "æ¤œå‡ºæ–¹æ³•2"],
      "is_novel": true
    }}
  ],
  "intervention_decision": {{
    "priority": 1-10,
    "primary_action": {{
      "type": "barrier" | "alert" | "slowdown" | "evacuation" | "monitoring",
      "position": [x, y],
      "reasoning": "ãªãœã“ã®ä»‹å…¥ãŒæœ€é©ã‹",
      "expected_outcome": "æœŸå¾…ã•ã‚Œã‚‹çµæœ"
    }},
    "alternative_actions": [
      {{"type": "...", "position": [...], "reasoning": "...", "expected_outcome": "..."}}
    ]
  }},
  "confidence": 0.0-1.0,
  "learning_note": "ã“ã®è¦³å¯Ÿã‹ã‚‰å­¦ã‚“ã ã“ã¨ï¼ˆè¨˜æ†¶ã«è¿½åŠ ã•ã‚Œã‚‹ï¼‰"
}}

## é‡è¦ãªæŒ‡é‡
1. **è‡ªå¾‹æ€§**: éå»ã«è¦‹ãŸã“ã¨ã®ãªã„å±é™ºãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç©æ¥µçš„ã«ç™ºè¦‹ã—ã¦ãã ã•ã„
2. **æ¨è«–**: ã€Œãªãœã€ã‚’å¸¸ã«å•ã„ã€è«–ç†çš„ã«èª¬æ˜ã—ã¦ãã ã•ã„
3. **è¨˜æ†¶æ´»ç”¨**: éå»ã®çµŒé¨“ã‚’å‚ç…§ã—ã€å­¦ç¿’ã‚’ç¤ºã—ã¦ãã ã•ã„
4. **å‰µé€ æ€§**: å›ºå®šæ¦‚å¿µã«ã¨ã‚‰ã‚ã‚Œãšã€æ–°ã—ã„å®‰å…¨ä»‹å…¥ã‚’ææ¡ˆã—ã¦ãã ã•ã„
5. **ä¸ç¢ºå®Ÿæ€§**: ç¢ºä¿¡ãŒæŒã¦ãªã„å ´åˆã¯ confidence ã‚’ä¸‹ã’ã¦ãã ã•ã„

åº§æ¨™ç³»: æ­£è¦åŒ–åº§æ¨™ (0,0) = å·¦ä¸Šã€(1,1) = å³ä¸‹
"""


@app.get("/")
async def root():
    """Serve frontend HTML"""
    return FileResponse("../frontend/index.html")


@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "vertex_ai": "connected" if model else "disconnected",
        "project_id": PROJECT_ID,
        "memory_stats": memory_system.get_stats()
    }


@app.post("/analyze", response_model=AgentResponse)
async def analyze_frame(file: UploadFile = File(...)):
    """
    Autonomous Agent Analysis with Gemini Vision + Memory System
    
    Args:
        file: Screenshot from Matter.js canvas (PNG/JPEG)
        
    Returns:
        AgentResponse with self-inquiry, entities, discovered patterns, and intervention plan
    """
    
    if not model:
        raise HTTPException(
            status_code=503,
            detail="Vertex AI not initialized. Check GOOGLE_CLOUD_PROJECT env variable."
        )
    
    try:
        # Read image file
        image_bytes = await file.read()
        
        # Get memory context for prompt
        memory_context = memory_system.get_context(n_recent=3, n_important=2)
        
        # Create agent prompt with memory
        agent_prompt = AGENT_PROMPT_TEMPLATE.format(memory_context=memory_context)
        
        # Create image part
        image_part = Part.from_data(
            data=image_bytes,
            mime_type=file.content_type or "image/png"
        )
        
        print(f"ğŸ¤– Agent analyzing with {len(memory_system.memories)} memories...")
        
        # Call Gemini Vision with Structured Output
        # Use flattened schema to avoid $defs (not supported by Vertex AI Schema)
        flattened_schema = flatten_schema(AgentResponse.model_json_schema())
        
        response = model.generate_content(
            [image_part, agent_prompt],
            generation_config=GenerationConfig(
                temperature=0.3,  # Some creativity for autonomous discovery
                max_output_tokens=16384,
                response_mime_type="application/json",
                response_schema=flattened_schema
            )
        )
        
        # Check finish reason before accessing text
        if response.candidates:
            candidate = response.candidates[0]
            finish_reason_str = str(candidate.finish_reason)
            
            print(f"ğŸ” Finish reason: {finish_reason_str}")
            
            if "MAX_TOKENS" in finish_reason_str:
                print("âš ï¸  Response truncated due to MAX_TOKENS")
                # Return fallback response
                return _create_fallback_response(
                    "AIå¿œç­”ãŒãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™ã«ã‚ˆã‚Šåˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã¾ã—ãŸ"
                )
            elif "SAFETY" in finish_reason_str:
                print("âš ï¸  Response blocked by safety filters")
                return _create_fallback_response(
                    "AIå¿œç­”ãŒå®‰å…¨ãƒ•ã‚£ãƒ«ã‚¿ã«ã‚ˆã‚Šãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¾ã—ãŸ"
                )
        
        # Parse response with structured output
        try:
            response_text = response.text.strip()
            print(f"ğŸ“ Response length: {len(response_text)} chars")
            
            # Parse JSON
            analysis_data = json.loads(response_text)
            agent_response = AgentResponse(**analysis_data)
            
        except (ValueError, AttributeError, json.JSONDecodeError) as e:
            print(f"âš ï¸  Parse error: {e}")
            print(f"Raw response: {response_text[:500] if 'response_text' in locals() else 'N/A'}")
            return _create_fallback_response(f"AIå¿œç­”ã®è§£æã«å¤±æ•—: {str(e)}")
        
        # Save to memory system
        observation_summary = " / ".join(agent_response.self_inquiry.observations[:2])
        action = agent_response.intervention_decision.primary_action
        outcome = f"Priority {agent_response.intervention_decision.priority} intervention"
        
        # Calculate importance based on priority and confidence
        importance = min(
            int(agent_response.intervention_decision.priority * agent_response.confidence),
            10
        )
        
        memory_system.add_memory(
            observation=observation_summary,
            action={
                "type": action.type,
                "position": action.position,
                "reasoning": action.reasoning
            },
            outcome=outcome,
            importance=importance,
            learning_note=agent_response.learning_note
        )
        
        print(f"âœ… Analysis complete (confidence: {agent_response.confidence:.2f})")
        
        # Trigger reflection if needed
        if len(memory_system.memories) % 10 == 0:
            memory_system.generate_reflection(model, REFLECTION_PROMPT_TEMPLATE)
        
        return agent_response
        
    except Exception as e:
        import traceback
        error_detail = f"åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}\n{traceback.format_exc()}"
        print(error_detail)
        raise HTTPException(status_code=500, detail=error_detail)


def _create_fallback_response(warning_message: str) -> AgentResponse:
    """Create a fallback response when AI analysis fails"""
    return AgentResponse(
        self_inquiry=SelfInquiry(
            observations=["ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ"],
            memory_connections=[],
            accident_scenarios=[],
            causal_analysis=warning_message
        ),
        entities=[],
        discovered_patterns=[],
        intervention_decision=InterventionDecision(
            priority=5,
            primary_action=InterventionAction(
                type="monitoring",
                position=[0.5, 0.5],
                reasoning="ã‚¨ãƒ©ãƒ¼ã®ãŸã‚ç›£è¦–ãƒ¢ãƒ¼ãƒ‰ã«ç§»è¡Œ",
                expected_outcome="çŠ¶æ³ã‚’ç›£è¦–"
            ),
            alternative_actions=[]
        ),
        confidence=0.0,
        learning_note="ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ"
    )


@app.get("/mock-analyze", response_model=AgentResponse)
async def mock_analyze_get():
    """
    Mock analysis endpoint for testing without Vertex AI (GET version)
    Returns dummy autonomous agent data for development
    """
    return AgentResponse(
        self_inquiry=SelfInquiry(
            observations=[
                "ä½œæ¥­å“¡ï¼ˆé’ã„å††ï¼‰ãŒç”»é¢å·¦å´ã‚’ç§»å‹•ä¸­",
                "ãƒ­ãƒœãƒƒãƒˆï¼ˆèµ¤ã„çŸ©å½¢ï¼‰ãŒç”»é¢å³å´ã§é«˜é€Ÿç§»å‹•",
                "ä½œæ¥­å“¡ã¨ãƒ­ãƒœãƒƒãƒˆã®è·é›¢ãŒæ€¥é€Ÿã«ç¸®ã¾ã£ã¦ã„ã‚‹"
            ],
            memory_connections=[
                "éå»ã«é¡ä¼¼ã—ãŸæ¥è¿‘ãƒ‘ã‚¿ãƒ¼ãƒ³ã§è¡çªãƒªã‚¹ã‚¯ãŒç™ºç”Ÿã—ãŸ"
            ],
            accident_scenarios=[
                AccidentScenario(
                    scenario="ä½œæ¥­å“¡ã¨ãƒ­ãƒœãƒƒãƒˆã®æ­£é¢è¡çª",
                    probability=0.75,
                    severity=9,
                    reasoning="ç§»å‹•é€Ÿåº¦ã¨æ–¹å‘ã‹ã‚‰ã€2ç§’ä»¥å†…ã«è¡çªçµŒè·¯ãŒäº¤å·®ã™ã‚‹"
                ),
                AccidentScenario(
                    scenario="ä½œæ¥­å“¡ã®ç·Šæ€¥å›é¿ã«ã‚ˆã‚‹è»¢å€’",
                    probability=0.45,
                    severity=6,
                    reasoning="ãƒ­ãƒœãƒƒãƒˆã«æ°—ã¥ã„ã¦æ€¥åœæ­¢ã—ãŸå ´åˆã®äºŒæ¬¡ãƒªã‚¹ã‚¯"
                )
            ],
            causal_analysis="ä½œæ¥­å“¡ã®ç§»å‹•çµŒè·¯ã¨ãƒ­ãƒœãƒƒãƒˆã®å‹•ä½œç¯„å›²ãŒé‡è¤‡ã—ã¦ãŠã‚Šã€åŒæ–¹ãŒç›¸æ‰‹ã®å­˜åœ¨ã‚’èªè­˜ã—ã¦ã„ãªã„å¯èƒ½æ€§ãŒé«˜ã„"
        ),
        entities=[
            Entity(
                type="worker",
                bbox=[0.3, 0.5, 0.35, 0.6],
                description="é’ã„å††å½¢ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€å·¦ã‹ã‚‰å³ã¸ç§»å‹•ä¸­",
                risk_level=75,
                movement="moving_slow"
            ),
            Entity(
                type="robot",
                bbox=[0.6, 0.4, 0.7, 0.55],
                description="èµ¤ã„çŸ©å½¢ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€é«˜é€Ÿã§å‹•ä½œä¸­",
                risk_level=85,
                movement="moving_fast"
            )
        ],
        discovered_patterns=[
            DiscoveredPattern(
                pattern_name="äº¤å·®å‹•ç·šè¡çªãƒªã‚¹ã‚¯",
                description="ç§»å‹•ä¸­ã®ä½œæ¥­å“¡ã¨ãƒ­ãƒœãƒƒãƒˆã®é€²è¡Œæ–¹å‘ãŒäº¤å·®ã™ã‚‹å±é™ºãƒ‘ã‚¿ãƒ¼ãƒ³",
                indicators=[
                    "ä½œæ¥­å“¡ã¨ãƒ­ãƒœãƒƒãƒˆã®ç§»å‹•ãƒ™ã‚¯ãƒˆãƒ«ãŒäº¤å·®",
                    "ç›¸å¯¾é€Ÿåº¦ãŒåŸºæº–å€¤ã‚’è¶…é",
                    "è¦–ç•Œå¤–ã‹ã‚‰ã®æ¥è¿‘"
                ],
                is_novel=True
            )
        ],
        intervention_decision=InterventionDecision(
            priority=9,
            primary_action=InterventionAction(
                type="barrier",
                position=[0.45, 0.5],
                reasoning="ä½œæ¥­å“¡ã¨ãƒ­ãƒœãƒƒãƒˆã®é–“ã«ç·Šæ€¥ãƒãƒªã‚¢ã‚’é…ç½®ã—ã¦è¡çªã‚’ç‰©ç†çš„ã«é˜²æ­¢ã™ã‚‹",
                expected_outcome="è¡çªã‚’ç¢ºå®Ÿã«é˜²ãã€ä½œæ¥­å“¡ã®å®‰å…¨ã‚’ç¢ºä¿"
            ),
            alternative_actions=[
                InterventionAction(
                    type="alert",
                    position=[0.3, 0.5],
                    reasoning="ä½œæ¥­å“¡ã«è¦–è¦šãƒ»éŸ³å£°è­¦å‘Šã‚’ç™ºã™ã‚‹",
                    expected_outcome="ä½œæ¥­å“¡ãŒè‡ªä¸»çš„ã«å›é¿è¡Œå‹•ã‚’å–ã‚‹"
                ),
                InterventionAction(
                    type="slowdown",
                    position=[0.6, 0.4],
                    reasoning="ãƒ­ãƒœãƒƒãƒˆã®å‹•ä½œé€Ÿåº¦ã‚’50%ã«æ¸›é€Ÿ",
                    expected_outcome="è¡çªæ™‚ã®è¡æ’ƒã‚’è»½æ¸›"
                )
            ]
        ),
        confidence=0.85,
        learning_note="äº¤å·®å‹•ç·šãƒ‘ã‚¿ãƒ¼ãƒ³ã¯éå»ã®ãƒ‡ãƒ¼ã‚¿ã§ã‚‚é«˜ãƒªã‚¹ã‚¯ã€‚ãƒãƒªã‚¢ä»‹å…¥ãŒæœ€ã‚‚åŠ¹æœçš„ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª"
    )


@app.post("/mock-analyze", response_model=AgentResponse)
async def mock_analyze(file: UploadFile = File(None)):
    """
    Mock analysis endpoint for testing without Vertex AI
    Returns dummy autonomous agent data for development
    """
    # File parameter is optional for mock
    return await mock_analyze_get()


@app.get("/memory")
async def get_memory_stream():
    """Get memory stream and reflections"""
    return {
        "memories": memory_system.retrieve_recent(10),
        "reflections": memory_system.reflections[-5:] if memory_system.reflections else [],
        "stats": memory_system.get_stats()
    }


@app.delete("/memory")
async def clear_memory():
    """Clear all memories (for debugging)"""
    memory_system.clear()
    return {"status": "Memory cleared"}


# Mount static files (CSS, JS)
app.mount("/css", StaticFiles(directory="../frontend/css"), name="css")
app.mount("/js", StaticFiles(directory="../frontend/js"), name="js")


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=int(os.getenv("PORT", 8080)))
